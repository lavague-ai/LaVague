{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LaVague for QA Automation\n",
    "\n",
    "In this notebook, we'll show how LaVague can be used to automatically generate pytest files from a Gherkin test definition\n",
    "\n",
    "We will use LaVague to autonomously run the test and record xpath and actions. We'll then use an LLM to generate assert statements and the final reusable test file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases\n",
    "**There are currently two examples test cases, only run the cell you want to test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope packaging CLI\n",
    "\n",
    "\n",
    "copier coller de lavague tests avec \n",
    "\n",
    "/lavague-qa/\n",
    "    examples/\n",
    "        demo_amazon.yaml\n",
    "            URL\n",
    "            GHERKIN\n",
    "    src/\n",
    "\n",
    "```\n",
    "lavague-qa -url https://example.com -feature path/to/feature/file\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case 1: Amazon - ecommerce cart feature\n",
    "\n",
    "LaVague can help you add test coverage for critical parts of your websites such as a Cart feature for ecommerce. \n",
    "\n",
    "- Accept cookies\n",
    "- Perform a search\n",
    "- Click on a product\n",
    "- Add it to cart\n",
    "- Access the cart page\n",
    "- Remove the item from cart\n",
    "- Verify that the cart is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.amazon.fr/\"\n",
    "\n",
    "GHERKIN = \"\"\"Feature: Cart\n",
    "\n",
    "  As a user, \n",
    "  I want, \n",
    "  So that, \n",
    "\n",
    "  Scenario: Add and remove a single product from cart\n",
    "    Given I am on the homepage\n",
    "    When I click \"Accepter\" to accept cookies\n",
    "    And I enter \"Zero to One\" into the search bar and press Enter\n",
    "    And I click on the first product in the search results\n",
    "    And I click on the \"Ajouter au panier\" button\n",
    "    And I the confirmation message has been displayed\n",
    "    And I click on \"Aller au panier\" under \"Passer la commande\"\n",
    "    And I click on \"Supprimer\" from the cart page\n",
    "    Then the cart should be empty\n",
    "\"\"\"\n",
    "\n",
    "FEATURE_FILE_NAME = \"demo_amazon.feature\"\n",
    "TEST_FILE_NAME = \"demo_amazon.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case 2: La Poste - interactive components\n",
    "\n",
    "LaVague can test the proper behavior of interactive elements on your site\n",
    "\n",
    "- Accept cookies\n",
    "- Navigate to the shipping calculator\n",
    "- Select values for package size and weight\n",
    "- Verify the price is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.laposte.fr/\"\n",
    "\n",
    "GHERKIN = \"\"\"Feature: Shipping cost calculator\n",
    "\n",
    "  Scenario: Estimate shipping costs for a large package\n",
    "    Given I am on the homepage\n",
    "    When I click on \"J'accepte\" to accept cookies\n",
    "    And I click on \"Envoyer un colis\"\n",
    "    And I click on the \"Format du colis\" dropdown under \"Dimension\"\n",
    "    And I click on \"Volumineux & tube\" from the dropdown results\n",
    "    And I enter 15 in the \"Poids\" field\n",
    "    And I wait for the cost to update\n",
    "    Then the cost should be \"34,70 €\"\n",
    "\"\"\"\n",
    "\n",
    "FEATURE_FILE_NAME = \"demo_laposte.feature\"\n",
    "TEST_FILE_NAME = \"demo_laposte.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case 3: Wikipedia login\n",
    "\n",
    "LaVague can test login features with some provided credentials\n",
    "\n",
    "- Accept cookies\n",
    "- Go to login\n",
    "- Login using credentials\n",
    "- Verifiy that login was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://fr.wikipedia.org/\"\n",
    "\n",
    "GHERKIN = \"\"\"Feature: Wikipedia Login\n",
    "\n",
    "  Scenario: User logs in successfully\n",
    "    Given the user is on the Wikipedia homepage\n",
    "    When the user navigates to the login page\n",
    "    And the user enters Lavague-test in the username field\n",
    "    And the user enters lavaguetest123 in the password field\n",
    "    And the user submits the login form\n",
    "    Then the login should be successful and the user is redirected to the main page\n",
    "\"\"\"\n",
    "\n",
    "FEATURE_FILE_NAME = \"demo_wikipedia.feature\"\n",
    "TEST_FILE_NAME = \"demo_wikipedia.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case 4: HSBC Navigation\n",
    "\n",
    "LaVague can make sure that links on your sites are not broken by checking multiple tabs if required\n",
    "\n",
    "- Go on homepage\n",
    "- Accept cookies\n",
    "- Click on a redirection link\n",
    "- Accept redirection\n",
    "- Switch to tab \n",
    "- Accept cookies\n",
    "- Navigate to another page\n",
    "- Check if expected page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.hsbc.fr/\"\n",
    "\n",
    "GHERKIN = \"\"\"Feature: HSBC navigation\n",
    "\n",
    "  Scenario: Multi tab navigation\n",
    "    Given the user is on the HSBC homepage\n",
    "    When the user clicks on \"Tout accepter\" to accept cookies\n",
    "    And the user clicks on \"Global Banking and Markets\"\n",
    "    And the user clicks on \"Je comprends, continuons\"\n",
    "    And the user navigates to the new tab opened\n",
    "    And the user clicks on \"Accept all cookies\"\n",
    "    And the user clicks on \"About us\"\n",
    "    Then the user should be on the \"About us\" page of the \"Global Banking and Markets\" services of HSBC\n",
    "\"\"\"\n",
    "\n",
    "FEATURE_FILE_NAME = \"demo_hsbc.feature\"\n",
    "TEST_FILE_NAME = \"demo_hsbc.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POC: End to end Gherkin generation\n",
    "\n",
    "We'll give LaVague the task of crawling the site and coming up with test cases in the Gherkin format that we will generate code for. This way we can add coverage from high level instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.axa.fr/\"\n",
    "# OBJECTIVE = \"Output one valid Gherkin file containing three scenarios for one feature to be tested on this site. Test steps should be precise and site specific. Keep the tests simple and make sure \"\n",
    "OBJECTIVE = \"Output one valid Gherkin file containing three scenarios for one feature to be tested on this site. The feature you'll be outputting tests for should require no real world data and should be testable with synthetic data only. The three assert statements should be testable from the same end state for all scenarios. You can navigate to subpages to make sure you output precise site specific test steps.\"\n",
    "\n",
    "FEATURE_FILE_NAME = \"demo_endtoend.feature\"\n",
    "TEST_FILE_NAME = \"demo_endtoend.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavague.core import  WorldModel, ActionEngine\n",
    "from lavague.core.agents import WebAgent\n",
    "from lavague.drivers.selenium import SeleniumDriver\n",
    "from lavague.core.retrievers import SemanticRetriever\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.legacy.readers.file.base import SimpleDirectoryReader\n",
    "\n",
    "import time\n",
    "\n",
    "selenium_driver = SeleniumDriver(headless=False)\n",
    "world_model = WorldModel()\n",
    "action_engine = ActionEngine(selenium_driver)\n",
    "agent = WebAgent(world_model, action_engine)\n",
    "\n",
    "agent.get(URL)\n",
    "time.sleep(3)\n",
    "\n",
    "agent_output = agent.run(OBJECTIVE, log_to_db=True)\n",
    "\n",
    "logs = agent.logger.return_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_output(markdown_code_block):\n",
    "    return markdown_code_block.replace(\"```gherkin\", \"\").replace(\"```\", \"\").replace(\"```\\n\", \"\").replace(\"gherkin\", \"\")\n",
    "\n",
    "GHERKIN = clean_output(agent_output.output)\n",
    "\n",
    "GHERKIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_5 = OpenAI(\"gpt-3.5-turbo\")\n",
    "output = gpt3_5.complete(\"Output a valid Gherkin file surrounded by triple quotes from this: \" + GHERKIN, max_tokens=500).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_output(markdown_code_block):\n",
    "    return markdown_code_block.replace(\"```gherkin\", \"\").replace(\"```\", \"\").replace(\"```\\n\", \"\")\n",
    "\n",
    "print(clean_output(output))\n",
    "GHERKIN = clean_output(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://temp-mail.org/en/\"\n",
    "\n",
    "GHERKIN = \"\"\"Get a temporary email address from this website then go on https://wwws.airfrance.fr/ and attempt to create an account. Go back to the temp-mail website and verify the email address if needed. \n",
    "\"\"\"\n",
    "\n",
    "FEATURE_FILE_NAME = \"demo_airfrance.feature\"\n",
    "TEST_FILE_NAME = \"demo_airfrance.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a test case with LaVague"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Gherkin\n",
    "We parse the Gherkin file to extract the assert statement, this will be usefull when we want to generate the assert code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenarios = GHERKIN.split(\"Scenario:\")\n",
    "# parsed_scenarios = []\n",
    "# for scenario in scenarios[1:]:\n",
    "#     scenario_name, *scenario_steps = scenario.strip().split(\"\\n\")\n",
    "#     parsed_scenarios.append(\n",
    "#         {\n",
    "#             \"name\": scenario_name.strip(),\n",
    "#             \"steps\": [step.strip() for step in scenario_steps],\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "# def prepare_scenario(scenario):\n",
    "#     return scenario[\"steps\"], scenario[\"steps\"][-1]\n",
    "\n",
    "\n",
    "# steps, assert_statement = prepare_scenario(parsed_scenarios[0])\n",
    "\n",
    "# print(assert_statement)\n",
    "# print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = GHERKIN.split(\"Scenario:\")\n",
    "parsed_scenarios = []\n",
    "for scenario in scenarios[1:]:\n",
    "    scenario_name, *scenario_steps = scenario.strip().split(\"\\n\")\n",
    "    parsed_scenarios.append(\n",
    "        {\n",
    "            \"name\": scenario_name.strip(),\n",
    "            \"steps\": [step.strip() for step in scenario_steps],\n",
    "        }\n",
    "    )\n",
    "\n",
    "def prepare_scenario(scenario):\n",
    "    return scenario[\"steps\"], scenario[\"steps\"][-1]\n",
    "\n",
    "assert_statements =  [scenario['steps'][-1] for scenario in parsed_scenarios]\n",
    "print(assert_statements)\n",
    "print(parsed_scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LaVague\n",
    "\n",
    "We create a standard LaVague agent and open the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavague.core import  WorldModel, ActionEngine\n",
    "from lavague.core.agents import WebAgent\n",
    "from lavague.drivers.selenium import SeleniumDriver\n",
    "from lavague.core.retrievers import SemanticRetriever\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.legacy.readers.file.base import SimpleDirectoryReader\n",
    "\n",
    "selenium_driver = SeleniumDriver(headless=False)\n",
    "world_model = WorldModel()\n",
    "action_engine = ActionEngine(selenium_driver)\n",
    "agent = WebAgent(world_model, action_engine)\n",
    "\n",
    "agent.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the agent\n",
    "We start the agent and record the steps taken as a `pandas` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = \"Run these scenarios step by step, make sure you complete each step: \" + GHERKIN\n",
    "# objective = GHERKIN\n",
    "# # agent.get(\"https://wwws.airfrance.fr/\")\n",
    "agent.run(objective, log_to_db=True)\n",
    "\n",
    "logs = agent.logger.return_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing a test case\n",
    "\n",
    "## Extract run steps and code\n",
    "After the agent has finished running, we extract instructions and actions taken from the logs. \n",
    "We also get the screenshot of the last page visited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will remove chain of thought comments to only keep the actions\n",
    "def remove_comments(code):\n",
    "    return '\\n'.join([line for line in code.split('\\n') if not line.strip().startswith('#')])\n",
    "\n",
    "# get all actions\n",
    "actions = \"\\n\".join(logs[\"code\"].dropna())\n",
    "\n",
    "# clean data\n",
    "logs['action'] = logs['code'].dropna().apply(remove_comments)\n",
    "cleaned_logs = logs[['instruction', 'action']].fillna('')\n",
    "actions = '\\n\\n'.join(cleaned_logs['instruction'] + ' ' + cleaned_logs['action'])\n",
    "\n",
    "# get last page screenshot\n",
    "last_page_screenshot = SimpleDirectoryReader(logs.iloc[-1][\"screenshots_path\"]).load_data() # load last screenshot taken\n",
    "\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve nodes for assert\n",
    "Using the `SemanticRetriever`, we fetch nodes that could be relevant to generate the assert statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tester xpathed_only à false\n",
    "retriever = SemanticRetriever(embedding=action_engine.python_engine.embedding, xpathed_only=False)\n",
    "html = selenium_driver.get_html()\n",
    "nodes = retriever.retrieve(f\"{assert_statements}\" , html.splitlines())\n",
    "\n",
    "# evolution potentielle pour + de robustesse dans la génération d'assert: \n",
    "# - identifier le Xpath de l'element sur lequel faire l'assert (ajouter un autre retriever avant le SemanticRetriever)\n",
    "# - modifier la pipeline retrieval pour tagger chaque element avec un xpath\n",
    "# - passer ca dans un LLM + le assert pour identifier le xpath de l'element sur lequel faire l'assert. \n",
    "\n",
    "\n",
    "# tag all nodes with xpath -> semantic retrieve on the assert statement -> extract xpath from the node that matches the assert statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can run this cell to display all nodes returned by the retriever\n",
    "from IPython.display import HTML\n",
    "\n",
    "for e in nodes: \n",
    "    display(HTML(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the `pytest` file\n",
    "Use recorded data about the site to generate a pytest-bdd file\n",
    "\n",
    "We use a multi modal LLM (`gpt-4o`) to generate the final code file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o = OpenAIMultiModal(\"gpt-4o\")\n",
    "gpt4o.max_new_tokens = 2000 # 300 by default, we increase it to make sure our pytest file doesn't get trucated\n",
    "\n",
    "# we'll clean the triple quoted answers from the LLM\n",
    "def clean_output(markdown_code_block):\n",
    "    return markdown_code_block.replace(\"```python\", \"\").replace(\"```\", \"\").replace(\"```\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the prompt\n",
    "\n",
    "We use a prompt that combines general instructions, examples and the recorded run data to generate the pytest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_PYTEST = \"\"\"import pytest\n",
    "from pytest_bdd import scenarios, given, when, then, parsers\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "BASE_URL = 'https://example.com'\n",
    "\n",
    "# Scenarios\n",
    "scenarios('complex_example.feature')\n",
    "\n",
    "# Fixtures\n",
    "@pytest.fixture\n",
    "def browser():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(10)\n",
    "    yield driver\n",
    "    driver.quit()\n",
    "\n",
    "# Steps\n",
    "@given('I am on the example website')\n",
    "def go_to_homepage(browser):\n",
    "    browser.get(BASE_URL)\n",
    "\n",
    "@when('I navigate to the product catalog')\n",
    "def navigate_to_catalog(browser):\n",
    "    catalog_link = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/header/nav/ul/li[3]/a\"))\n",
    "    )\n",
    "    try:\n",
    "        browser.execute_script(\"arguments[0].click();\", catalog_link)\n",
    "    except ElementClickInterceptedException:\n",
    "        pytest.fail(\"Failed to navigate to the product catalog\")\n",
    "\n",
    "@when('I filter products by category')\n",
    "def filter_products(browser):\n",
    "    category_dropdown = WebDriverWait(browser, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"/html/body/div[2]/main/div/div[1]/aside/div[3]/select\"))\n",
    "    )\n",
    "    options = category_dropdown.find_elements(By.TAG_NAME, \"option\")\n",
    "    random_option = random.choice(options[1:])  # Exclude the first option if it's a placeholder\n",
    "    random_option.click()\n",
    "\n",
    "@when('I sort products by price')\n",
    "def sort_products(browser):\n",
    "    sort_button = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"/html/body/div[2]/main/div/div[2]/div[1]/div/button[2]\"))\n",
    "    )\n",
    "    try:\n",
    "        browser.execute_script(\"arguments[0].click();\", sort_button)\n",
    "    except ElementClickInterceptedException:\n",
    "        pytest.fail(\"Failed to sort products\")\n",
    "\n",
    "@when('I add a random product to the cart')\n",
    "def add_to_cart(browser):\n",
    "    products = browser.find_elements(By.XPATH, \"/html/body/div[2]/main/div/div[2]/ul/li\")\n",
    "    random_product = random.choice(products)\n",
    "    add_to_cart_button = random_product.find_element(By.XPATH, \".//button[@data-testid='add-to-cart']\")\n",
    "    try:\n",
    "        browser.execute_script(\"arguments[0].click();\", add_to_cart_button)\n",
    "    except ElementClickInterceptedException:\n",
    "        pytest.fail(\"Failed to add product to cart\")\n",
    "\n",
    "@when('I proceed to checkout')\n",
    "def proceed_to_checkout(browser):\n",
    "    checkout_button = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/header/div[2]/div/div/a\"))\n",
    "    )\n",
    "    try:\n",
    "        browser.execute_script(\"arguments[0].click();\", checkout_button)\n",
    "    except ElementClickInterceptedException:\n",
    "        pytest.fail(\"Failed to proceed to checkout\")\n",
    "\n",
    "@then('I should see the checkout form')\n",
    "def verify_checkout_form(browser):\n",
    "    try:\n",
    "        WebDriverWait(browser, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"checkout-form\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Checkout form not found: {str(e)}\")\n",
    "\n",
    "@then('the cart total should be correct')\n",
    "def verify_cart_total(browser):\n",
    "    try:\n",
    "        total_element = WebDriverWait(browser, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"cart-total\"))\n",
    "        )\n",
    "        total_value = float(total_element.text.replace('$', ''))\n",
    "        assert total_value > 0, \"Cart total should be greater than zero\"\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Failed to verify cart total: {str(e)}\")\n",
    "\n",
    "@then('the product list should be visible')\n",
    "def verify_product_list(browser):\n",
    "    try:\n",
    "        WebDriverWait(browser, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"product-list\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Product list not found: {str(e)}\")\n",
    "\n",
    "@then('the category filter should be available')\n",
    "def verify_category_filter(browser):\n",
    "    try:\n",
    "        filter_element = WebDriverWait(browser, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"category-filter\"))\n",
    "        )\n",
    "        assert filter_element.is_enabled(), \"Category filter should be enabled\"\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Category filter not found or not enabled: {str(e)}\")\n",
    "\n",
    "@then('the \"Add to Cart\" button should be present for each product')\n",
    "def verify_add_to_cart_buttons(browser):\n",
    "    try:\n",
    "        add_to_cart_buttons = WebDriverWait(browser, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"button[data-testid='add-to-cart']\"))\n",
    "        )\n",
    "        assert len(add_to_cart_buttons) > 0, \"No 'Add to Cart' buttons found\"\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Failed to verify 'Add to Cart' buttons: {str(e)}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = f\"\"\"\n",
    "You are an expert in software testing frameworks and Python code generation. You answer in python markdown only and nothing else.\n",
    "Your only goal is to generate pytest-bdd files based on the provided Gherkin feature, a collection of instructions and actions, and a specific assert statement to test.\n",
    "You will use the provided information to generate a valid assert statement. \n",
    "- Name the scenario appropriately.\n",
    "- Always use time.sleep(3) if waiting is required.\n",
    "- Include all necessary imports and fixtures.\n",
    "- Use provided actions to find valid XPath selectors for the final pytest file. \n",
    "- You answer in python code only and nothing else.\n",
    "\n",
    "I will provide an example below:\n",
    "----------\n",
    "Feature file name: example.feature\n",
    "\n",
    "URL: https://www.example.com\n",
    "\n",
    "Gherkin of the feature to be tested: \n",
    "\n",
    "Feature: E-commerce Website Interaction\n",
    "\n",
    "  Scenario: Browse products and checkout\n",
    "    Given I am on the example website\n",
    "    When I navigate to the product catalog\n",
    "    And I filter products by category\n",
    "    And I sort products by price\n",
    "    And I add a random product to the cart\n",
    "    And I proceed to checkout\n",
    "    Then the cart total should be correct\n",
    "\n",
    "Assert statement Then the cart total should be correct\n",
    "\n",
    "List of already executed instructions and actions:\n",
    "- instruction: Navigate to the product catalog page\n",
    "  actions:\n",
    "    - action:\n",
    "        name: \"click\"\n",
    "        args:\n",
    "          xpath: \"/html/body/div[1]/header/nav/ul/li[3]/a\"\n",
    "          value: \"\"\n",
    "\n",
    "- instruction: Select a category from the dropdown filter\n",
    "  actions:\n",
    "    - action:\n",
    "        name: \"click\"\n",
    "        args:\n",
    "          xpath: \"/html/body/div[2]/main/div/div[1]/aside/div[3]/select/option[3]\"\n",
    "          value: \"\"\n",
    "\n",
    "- instruction: Sort products by price\n",
    "  actions:\n",
    "    - action:\n",
    "        name: \"click\"\n",
    "        args:\n",
    "          xpath: \"/html/body/div[2]/main/div/div[2]/div[1]/div/button[2]\"\n",
    "          value: \"\"\n",
    "\n",
    "- instruction: Add a random product to the cart\n",
    "  actions:\n",
    "    - action:\n",
    "        name: \"click\"\n",
    "        args:\n",
    "          xpath: \"/html/body/div[2]/main/div/div[2]/ul/li[5]/div/button[@data-testid='add-to-cart']\"\n",
    "          value: \"\"\n",
    "\n",
    "- instruction: Proceed to checkout\n",
    "  actions:\n",
    "    - action:\n",
    "        name: \"click\"\n",
    "        args:\n",
    "          xpath: \"/html/body/div[1]/header/div[2]/div/div/a\"\n",
    "          value: \"\"\n",
    "          \n",
    "          \n",
    "Resulting pytest code: \n",
    "{EXAMPLE_PYTEST}\n",
    "\n",
    "----------\n",
    "Given this information, generate a valid pytest-bdd file with the following inputs:\n",
    "Feature file name: {FEATURE_FILE_NAME}\\n\n",
    "URL: {URL}\\n\n",
    "Gherkin of the feature to be tested:\n",
    "{GHERKIN}\\n\n",
    "Assert statement: {assert_statements}\\n\n",
    "Potentially relevant nodes that you may use to help you generate the assert code: {nodes}\\n\n",
    "List of already executed instructions and actions:\n",
    "{actions}\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the test\n",
    "Call the LLM with the final prompt then clean the ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_pytest = gpt4o.complete(PROMPT, image_documents=last_page_screenshot).text\n",
    "generated_pytest = clean_output(generated_pytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_pytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write test to file\n",
    "\n",
    "We'll write two generated files to disk: \n",
    "- `.feature` contains the test scenarios written in the Gherkin syntax\n",
    "- `.py` the actual automated test that we'll run with pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(FEATURE_FILE_NAME, \"w\") as file:\n",
    "        file.write(GHERKIN)\n",
    "        \n",
    "with open(TEST_FILE_NAME, \"w\") as file:\n",
    "        print(\"WRITING FILE\")\n",
    "        file.write(generated_pytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tests\n",
    "\n",
    "We can finally run our generated test to see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest demo_amazon.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest demo_laposte.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest demo_wikipedia.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest demo_hsbc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -v demo_endtoend.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavague-Oj4z07SL-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
