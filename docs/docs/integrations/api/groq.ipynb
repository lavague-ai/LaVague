{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95df10e0-5435-43c0-a941-71e8364a832d",
   "metadata": {},
   "source": [
    "<div id=\"colab_button\\\">\n",
    "    <h1>LaVague: Groq integration</h1>\n",
    "    <a target=\"_blank\\\" href=\"https://colab.research.google.com/github/lavague-ai/lavague/blob/main/docs/docs/integrations/api/groq.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc125ce-b5f1-403f-aa29-483db42a8a03",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is part of our integrations section which shows you how to quickly get started with different LLM integrations. In this notebook, we will use LaVague with Groq's Language Processing Unit (LPU) via the Groq API. The default model used is `llama3-70b-8192`, but you can also choose from other models such as `llama3-8b-8192`, `llama2-70b-4096`, `mixtral-8x7b-32768`, and `gemma-7b-it` by specifying the desired model in the configuration file.\n",
    "\n",
    "One of the key features of the Groq API is its ability to process up to 300 tokens per second per user using Groq's Language Processing Unit (LPU). This is at least 3 times and up to 18 times faster than traditional cloud-based models.\n",
    "\n",
    "**Pre-requisites:**\n",
    "\n",
    "- For our integration notebooks, we assume you have already downloaded the necessary webdriver folders and moved them to your root directory.\n",
    "\n",
    "- For our integration notebooks, we assume you have already downloaded and installed the latest `lavague` package from the `main` of our GitHub repo. For more information on installation - see our [installation guide](https://docs.lavague.ai/en/latest/docs/get-started/setting-up-la-vague/) or our [quick tour notebook](https://docs.lavague.ai/en/latest/docs/get-started/quick-tour/)\n",
    "\n",
    "- If you are running the notebook locally, you will need Python (tested on python>=3.8) and pip installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11615059-f3db-4d4f-8410-cc3aa796558d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "⚠️ For remote inference with the Groq API, you will need to provide your Groq key in the code block below.\n",
    "\n",
    "> If you don't have a Groq API key, you can get one by creating a Groq account and following the instructions [here](https://console.groq.com/docs/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57fd3e-dfad-4189-992f-1216dad84172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = # ADD YOUR Groq API KEY HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34618275-fb0c-4ff6-901c-6b34b6b2b65a",
   "metadata": {},
   "source": [
    "### LaVague Launch\n",
    "\n",
    "We will now download the default configuration files for running LaVague with the Groq API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5194e60-e30e-4348-9579-f124fa8ebac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/lavague-ai/LaVague/main/examples/configurations/api/groq_api.py\n",
    "!wget https://raw.githubusercontent.com/lavague-ai/LaVague/main/examples/instructions/huggingface.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256c83a-3864-4cc6-900d-929524d4b0f5",
   "metadata": {},
   "source": [
    "We can now launch our interactive Gradio which will be created with three default instructions which can be executed on the HuggingFace website as defined in the `huggingface.yaml` file.\n",
    "\n",
    "> Feel free to take a look at the `groq_api.py` file to see our default config and play around with different configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7633e-92ec-4c33-b12b-52f6f2aa5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lavague -i huggingface.yaml -c groq_api.py launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7b65e-d81f-4538-b6aa-7cce6e961b40",
   "metadata": {},
   "source": [
    "You can now click on the public (if you are using Google Colab) or local URL to open the Gradio in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2a0f2-f5a4-4220-bba3-5198436f5827",
   "metadata": {},
   "source": [
    "### LaVague Build\n",
    "\n",
    "We can alternatively use the `lavague [OPTIONS] build` command to generate a python script with the Selenium code needed to perform your desired action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9c14b-f2d2-4c58-bf01-5d0513958e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lavague -i huggingface.yaml -c groq_api.py build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c429c-5ea4-4e76-bf82-8ff068c597b5",
   "metadata": {},
   "source": [
    "This creates a script with the Python code generated by the LLM to perform the desired action in the current directory named `huggingface_groq_api_gen.py`.\n",
    "\n",
    "You can now inspect the code or execute it locally!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9776f5e-ec47-46b5-9e44-42f710bd7cc7",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "That brings us to the end of this OpenAI integration demo. \n",
    "\n",
    "If you have any further questions, join us on the LaVague Discord [here](https://discord.com/invite/SDxn9KpqX9)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavague_env",
   "language": "python",
   "name": "lavague_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
